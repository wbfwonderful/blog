<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="wbfwonderful">
    
    <title>
        
            【LLM】nanoGPT学习 |
        
        wbfwonderful&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/blog/css/style.css">

    
        <link rel="shortcut icon" href="https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile-transparent.png">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/font/css/fontawesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/font/css/regular.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/font/css/solid.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/font/css/brands.min.css">
    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"wbfwonderful.github.io","root":"/blog/","language":"zh-CN","path":"search.json"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"wbfwonderful's Blog","author":"wbfwonderful","avatar":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile.jpg","logo":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile.jpg","favicon":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile-transparent.png"},"menu":{"home":"/ || fa-solid fa-home","archives":"/archives || fa-solid fa-box-archive","tags":"/tags || fa-solid fa-tags","categories":"/categories || fa-solid fa-layer-group","photos":"/photos || fa-solid fa-image","about":"/about || fa-solid fa-user-graduate"},"first_screen":{"enable":true,"background_img":"/images/screen.gif","background_img_dark":"/images/screen.gif","description":null,"hitokoto":false},"social_contact":{"enable":true,"links":{"github":"https://github.com/wbfwonderful","weixin":null,"qq":null,"weibo":null,"zhihu":null,"twitter":null,"x":null,"facebook":null,"email":"wangbf23@mail2.sysu.edu.cn","bilibili":"https://space.bilibili.com/214019111"}},"scroll":{"progress_bar":false,"percent":false,"hide_header":false},"home":{"announcement":null,"category":true,"tag":true,"post_datetime":"created || fa-solid fa-arrow-up-from-bracket","post_datetime_format":"YYYY-MM-DD HH:mm:ss"},"post":{"author_badge":{"enable":true,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":true,"min2read":true},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":true,"share":true,"reward":{"enable":false,"img_link":null,"text":null,"icon":null},"created_datetime_icon":"fa-solid fa-arrow-up-from-bracket","updated_datetime_icon":"fa-solid fa-arrows-rotate"},"code_block":{"tools":{"enable":true,"style":"mac"},"highlight_theme":"default"},"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true,"layout":"right"},"website_count":{"busuanzi_count":{"enable":true,"site_uv":true,"site_pv":true,"page_pv":true}},"local_search":{"enable":true,"preload":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.39"},"waline":{"server_url":null,"reaction":false,"version":"3.3.2"},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":false},"lazyload":{"enable":true},"cdn":{"enable":true,"provider":"cdnjs"},"pjax":{"enable":false},"footer":{"since":2024,"word_count":true,"site_deploy":{"enable":true,"provider":"github","url":"https://github.com"},"record":{"enable":false,"list":[{"code":null,"link":null}]}},"inject":{"enable":false,"css":["/css/custom-photos.css"],"js":[null]},"root":"/blog","source_data":{"photos":[{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile.jpg","name":"头像"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/hainan-warship.jpg","name":"海南舰"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/mayday.jpg","name":"5525深圳站"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/balls.gif","name":"五月天五个球球"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/JJ20.jpg","name":"JJ20深圳站"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/HKDisney.jpg","name":"香港迪士尼"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/ktv.jpg","name":"笑忘歌"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/lights.jpg","name":"2025自贡灯会"},{"url":"https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/fireworks.jpg","name":"2025新年快乐！"}]},"version":"4.2.4"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>



<main class="page-container border-box">
    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left flex-start border-box">
            
                <a class="logo-image border-box" href="/blog/">
                    <img src="https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile.jpg">
                </a>
            
            <a class="site-name border-box" href="/blog/">
               wbfwonderful&#39;s Blog
            </a>
        </div>

        <div class="right border-box">
            <div class="pc border-box">
                <ul class="menu-list border-box">
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-home"></i>
                                
                                首页
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/archives">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                
                                归档
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/tags">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-tags"></i>
                                
                                标签
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/categories">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-layer-group"></i>
                                
                                分类
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/photos">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-image"></i>
                                
                                相册
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/blog/about">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-user-graduate"></i>
                                
                                关于
                                
                            </a>
                            
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="menu-text-color fas search fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile border-box flex-start">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list border-box">
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-home"></i>
                                </span>
                            
                            首页
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/archives">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                </span>
                            
                            归档
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/tags">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-tags"></i>
                                </span>
                            
                            标签
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/categories">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-layer-group"></i>
                                </span>
                            
                            分类
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/photos">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-image"></i>
                                </span>
                            
                            相册
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/blog/about">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-user-graduate"></i>
                                </span>
                            
                            关于
                        </a>
                        
                    </label>
                    
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        【LLM】nanoGPT学习
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/profile.jpg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">wbfwonderful</span>
                                
                                    <span class="author-badge">Lv4</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-arrow-up-from-bracket"></i>&nbsp;
                <span class="datetime">2025-02-23 14:06:02</span>
            </span>

            
                <span class="meta-info-item post-update-date">
                    <i class="icon fa-solid fa-arrows-rotate"></i>&nbsp;
                    <span class="datetime" data-updated="Sat Jun 14 2025 10:01:33 GMT+0800">2025-06-14 10:01:33</span>
                </span>
            
        

        
            <span class="meta-info-item post-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="post-category-ul">
                    
                            <li class="category-item"><a href="/blog/categories/LLM/">LLM</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="post-tag meta-info-item border-box">
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/blog/tags/LLM/">LLM</a></li>
                        
                    
                </ul>
            </span>
        

        
        
            <span class="meta-info-item post-wordcount">
                <i class="icon fas fa-file-word"></i>&nbsp;<span>2.2k 字</span>
            </span>
        
        
            <span class="meta-info-item post-min2read">
                <i class="icon fas fa-clock"></i>&nbsp;<span>10 分钟</span>
            </span>
        
        
            <span class="meta-info-item post-pv">
                <i class="icon fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
            </span>
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body ">
                    

                    
                         <h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>本文主要记录 nanoGPT 的学习过程，参考链接</p>
<blockquote>
<p>GPT in 60 Lines of NumPy：<a class="link"   target="_blank" rel="noopener" href="https://jaykmody.com/blog/gpt-from-scratch/" >https://jaykmody.com/blog/gpt-from-scratch/<i class="fas fa-external-link-alt"></i></a><br>60 行代码实现 gpt（上一篇的翻译）： <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/679330102" >https://zhuanlan.zhihu.com/p/679330102<i class="fas fa-external-link-alt"></i></a><br>nanoGPT 实战： <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/716442447" >https://zhuanlan.zhihu.com/p/716442447<i class="fas fa-external-link-alt"></i></a><br>nanoGPT 代码解读：<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677407971" >https://zhuanlan.zhihu.com/p/677407971<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<h1 id="GPT-原理"><a href="#GPT-原理" class="headerlink" title="GPT 原理"></a>GPT 原理</h1><ul>
<li>GPT(Generative Pre-trained Transformer)基于Transformer解码器自回归地<strong>预测下一个Token</strong>，从而进行了语言模型的建模。GPT的伪代码可以简单的表示为：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gpt</span>(<span class="params">inputs: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">list</span>[<span class="built_in">float</span>]]:</span><br><span class="line">	<span class="string">&quot;&quot;&quot; GPT代码，实现预测下一个token</span></span><br><span class="line"><span class="string">	inputs：List[int], shape为[n_seq]，输入文本序列的token id的列表</span></span><br><span class="line"><span class="string">	output：List[List[int]], shape为[n_seq, n_vocab]，预测输出的logits列表</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    output = <span class="comment"># 需要实现的GPT内部计算逻辑 </span></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<ul>
<li>即输入一段token</li>
</ul>
<h2 id="关于-token"><a href="#关于-token" class="headerlink" title="关于 token"></a>关于 token</h2><ul>
<li><p>token 可以理解为一个句子中最小的组成部分。通常为一个词，一些情况下，可以进行简化，例如后续使用莎士比亚的作品集进行训练时，将字符作为 token。</p>
</li>
<li><p>token 通过分词器来获取，对应一个词汇表。最开始输入到模型中的序列其实为一串数字，表示当前 token 在词汇表中的位置。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词汇表中的token索引表示该token的整数ID</span></span><br><span class="line"><span class="comment"># 例如，&quot;robot&quot;的整数ID为1，因为vocab[1] = &quot;robot&quot;</span></span><br><span class="line">vocab = [<span class="string">&quot;must&quot;</span>, <span class="string">&quot;robot&quot;</span>, <span class="string">&quot;obey&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;orders&quot;</span>, <span class="string">&quot;.&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行分词的分词器tokenizer（假设通过空格来进行分词）</span></span><br><span class="line">tokenizer = WhitespaceTokenizer(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># encode()方法将str字符串转换为list[int]</span></span><br><span class="line">ids = tokenizer.encode(<span class="string">&quot;robot must obey orders&quot;</span>) <span class="comment"># ids = [1, 0, 2, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过词汇表映射，可以看到实际的token是什么</span></span><br><span class="line">tokens = [tokenizer.vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids] <span class="comment"># tokens = [&quot;robot&quot;, &quot;must&quot;, &quot;obey&quot;, &quot;orders&quot;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># decode()方法将list[int] 转换回str</span></span><br><span class="line">text = tokenizer.decode(ids) <span class="comment"># text = &quot;robot must obey orders&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>同样，输出为一个二维数组，表示当前位置不同 token 的出现概率。output 是一个二维数组，其中 output[i][j] 表示文本序列的第 i 个位置的 token（inputs[i]）是词汇表的第 j 个 token（vocab[j]）的概率（实际为未归一化的logits得分）。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">inputs = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>]  <span class="comment"># &quot;robot&quot; &quot;must&quot; &quot;obey&quot; &quot;orders&quot;</span></span><br><span class="line">vocab = [<span class="string">&quot;must&quot;</span>, <span class="string">&quot;robot&quot;</span>, <span class="string">&quot;obey&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;orders&quot;</span>, <span class="string">&quot;.&quot;</span>]</span><br><span class="line">output = gpt(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output[0] = [0.75, 0.1, 0.15, 0.0, 0.0, 0.0]</span></span><br><span class="line"><span class="comment"># 给定 &quot;robot&quot;，模型预测 &quot;must&quot; 的概率最高</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output[1] = [0.0, 0.0, 0.8, 0.1, 0.0, 0.1]</span></span><br><span class="line"><span class="comment"># 给定序列 [&quot;robot&quot;, &quot;must&quot;]，模型预测 &quot;obey&quot; 的概率最高</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output[-1] = [0.0, 0.0, 0.1, 0.0, 0.85, 0.05]</span></span><br><span class="line"><span class="comment"># 给定整个序列[&quot;robot&quot;, &quot;must&quot;, &quot;obey&quot;]，模型预测 &quot;orders&quot; 的概率最高</span></span><br><span class="line">next_token_id = np.argmax(output[-<span class="number">1</span>])  <span class="comment"># next_token_id = 4</span></span><br><span class="line">next_token = vocab[next_token_id]      <span class="comment"># next_token = &quot;orders&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>在推理时（生成文本），首先将 prompt 输入 GPT，然后迭代地将上一轮的输出放到当前的末尾，重复生成。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">inputs, n_tokens_to_generate</span>):</span><br><span class="line">	<span class="string">&quot;&quot;&quot; GPT生成代码</span></span><br><span class="line"><span class="string">	inputs: list[int], 输入文本的token ids列表</span></span><br><span class="line"><span class="string">	n_tokens_to_generate：int, 需要生成的token数量</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 自回归式解码循环</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_tokens_to_generate): </span><br><span class="line">        output = gpt(inputs)            <span class="comment"># 模型前向推理，输出预测词表大小的logits列表</span></span><br><span class="line">        next_id = np.argmax(output[-<span class="number">1</span>]) <span class="comment"># 贪心采样</span></span><br><span class="line">        inputs.append(<span class="built_in">int</span>(next_id))     <span class="comment"># 将预测添加回输入</span></span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="built_in">len</span>(inputs) - n_tokens_to_generate :]  <span class="comment"># 只返回生成的ids</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随便举例</span></span><br><span class="line">input_ids = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]                          <span class="comment"># [&quot;robot&quot;, &quot;must&quot;, &quot;obey&quot;]</span></span><br><span class="line">output_ids = generate(input_ids, <span class="number">1</span>)            <span class="comment">#  output_ids = [1, 0, 2, 4]</span></span><br><span class="line">output_tokens = [vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> output_ids] <span class="comment"># [&quot;robot&quot;, &quot;must&quot;, &quot;obey&quot;, &quot;orders&quot;]</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>首先，最外层调用的是 GPT 类。调用方法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits, loss = model(X, Y)</span><br></pre></td></tr></table></figure>
<p>其中 X，Y 表示输入以及其对应的标签，注意这里已经为 int 类型的数组了（表示 token 在词汇表中的位置）。</p>
<h2 id="GPT-类"><a href="#GPT-类" class="headerlink" title="GPT 类"></a>GPT 类</h2><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idx, targets=<span class="literal">None</span></span>):</span><br><span class="line">    device = idx.device</span><br><span class="line">    b, t = idx.size()</span><br><span class="line">    <span class="keyword">assert</span> t &lt;= <span class="variable language_">self</span>.config.block_size, <span class="string">f&quot;Cannot forward sequence of length <span class="subst">&#123;t&#125;</span>, block size is only <span class="subst">&#123;self.config.block_size&#125;</span>&quot;</span></span><br><span class="line">    pos = torch.arange(<span class="number">0</span>, t, dtype=torch.long, device=device)</span><br><span class="line">    tok_emb = <span class="variable language_">self</span>.transformer.wte(idx) </span><br><span class="line">    pos_emb = <span class="variable language_">self</span>.transformer.wpe(pos) </span><br><span class="line">    x = <span class="variable language_">self</span>.transformer.drop(tok_emb + pos_emb)</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.transformer.h:</span><br><span class="line">        x = block(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.transformer.ln_f(x)</span><br><span class="line">    <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        logits = <span class="variable language_">self</span>.lm_head(x)</span><br><span class="line">        loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>), ignore_index=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logits = <span class="variable language_">self</span>.lm_head(x[:, [-<span class="number">1</span>], :]) <span class="comment"># note: using list [-1] to preserve the time dim</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> logits, loss</span><br></pre></td></tr></table></figure>
<ul>
<li>输入 idx 类型为 list[int]，表示输入 token 在词汇表中的索引，并且有 batch 维度。此处有一个断言，即要求序列的长度要小于块长度，即 block_size 表示模型能处理的最大长度。</li>
<li>新建一个位置数组 pso，用于计算位置编码。</li>
<li>接下来为核心代码 self.transformer，实现如下<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.transformer = nn.ModuleDict(<span class="built_in">dict</span>(</span><br><span class="line">    wte = nn.Embedding(config.vocab_size, config.n_embd),</span><br><span class="line">    wpe = nn.Embedding(config.block_size, config.n_embd),</span><br><span class="line">    drop = nn.Dropout(config.dropout),</span><br><span class="line">    h = nn.ModuleList([Block(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.n_layer)]),</span><br><span class="line">    ln_f = LayerNorm(config.n_embd, bias=config.bias),</span><br><span class="line">))</span><br></pre></td></tr></table></figure></li>
<li>其包含 5 个部分：token 编码（word token embedding, wte）、位置编码（word position embedding, wpe）、dropout、注意力块（block）和层归一化。</li>
<li>整体处理流程为：<ul>
<li>token 编码 + 位置编码，并相加</li>
<li>依次通过注意力层</li>
<li>经过最终的映射层（将注意力的输出映射到词汇表维度）</li>
<li>最后进行判断是否计算 loss</li>
</ul>
</li>
</ul>
<h3 id="token-编码和位置编码"><a href="#token-编码和位置编码" class="headerlink" title="token 编码和位置编码"></a>token 编码和位置编码</h3><ul>
<li><strong>token 编码</strong>：wte 是一个 [n_vocab, n_embd] 大小的可学习参数矩阵，它充当一个 token 嵌入<strong>查找表</strong>，其中矩阵的第 i 对应于词汇表中第 i 个 token 的 embedding。<ul>
<li>wte[idx] 使用 Token Ids 列表索引来<strong>检索</strong>与输入中每个token对应的向量。<br><img  
                       lazyload
                       alt="image"
                       data-src="https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/llm-wte.png"
                       
                 ></li>
</ul>
</li>
<li><strong>位置编码</strong>：表示序列的先后信息，同样是一个 [n_block, n_embd] 大小的可学习参数矩阵。<br><img  
                       lazyload
                       alt="image"
                       data-src="https://wbfwonderful.oss-cn-shenzhen.aliyuncs.com/llm-wpe.png"
                       
                 ></li>
</ul>
<h2 id="Block-类"><a href="#Block-类" class="headerlink" title="Block 类"></a>Block 类</h2><ul>
<li>Block 类的实现如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.ln_1 = LayerNorm(config.n_embd, bias=config.bias)</span><br><span class="line">        <span class="variable language_">self</span>.attn = CausalSelfAttention(config)</span><br><span class="line">        <span class="variable language_">self</span>.ln_2 = LayerNorm(config.n_embd, bias=config.bias)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = MLP(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.attn(<span class="variable language_">self</span>.ln_1(x))</span><br><span class="line">        x = x + <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li>其主要包含两个层归一化、MLP和注意力层。</li>
</ul>
<h2 id="CausalSelfAttention-类"><a href="#CausalSelfAttention-类" class="headerlink" title="CausalSelfAttention 类"></a>CausalSelfAttention 类</h2><p>实现注意力机制的核心类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    B, T, C = x.size() <span class="comment"># batch size, sequence length, embedding dimensionality (n_embd)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span><br><span class="line">    q, k, v  = <span class="variable language_">self</span>.c_attn(x).split(<span class="variable language_">self</span>.n_embd, dim=<span class="number">2</span>)</span><br><span class="line">    k = k.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line">    q = q.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line">    v = v.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.flash:</span><br><span class="line">        <span class="comment"># efficient attention using Flash Attention CUDA kernels</span></span><br><span class="line">        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=<span class="literal">None</span>, dropout_p=<span class="variable language_">self</span>.dropout <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">else</span> <span class="number">0</span>, is_causal=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># manual implementation of attention</span></span><br><span class="line">        att = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * (<span class="number">1.0</span> / math.sqrt(k.size(-<span class="number">1</span>)))</span><br><span class="line">        att = att.masked_fill(<span class="variable language_">self</span>.bias[:,:,:T,:T] == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        att = F.softmax(att, dim=-<span class="number">1</span>)</span><br><span class="line">        att = <span class="variable language_">self</span>.attn_dropout(att)</span><br><span class="line">        y = att @ v <span class="comment"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span></span><br><span class="line">    y = y.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(B, T, C) <span class="comment"># re-assemble all head outputs side by side</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># output projection</span></span><br><span class="line">    y = <span class="variable language_">self</span>.resid_dropout(<span class="variable language_">self</span>.c_proj(y))</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<ul>
<li><p>self.c_attn(x) 表示为注意力机制的映射层，并将 Q、K、V 三个映射层合并为一个，减少计算量。计算出映射矩阵后再进行划分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.c_attn = nn.Linear(config.n_embd, <span class="number">3</span> * config.n_embd, bias=config.bias)</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来根据 head 的数量对映射矩阵进行划分。然后就是计算注意力。</p>
</li>
<li><p>注意，为了实现<strong>因果</strong>机制，即模型只能看到当前 token 之前的 token，需要将计算出的 attn 矩阵 mask 一部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入是 [&quot;not&quot;, &quot;all&quot;, &quot;heroes&quot;, &quot;wear&quot;, &quot;capes&quot;] </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始自注意力</span></span><br><span class="line">        <span class="keyword">not</span>    <span class="built_in">all</span>   heroes  wear  capes</span><br><span class="line">   <span class="keyword">not</span> <span class="number">0.116</span>  <span class="number">0.159</span>  <span class="number">0.055</span>  <span class="number">0.226</span>  <span class="number">0.443</span></span><br><span class="line">   <span class="built_in">all</span> <span class="number">0.180</span>  <span class="number">0.397</span>  <span class="number">0.142</span>  <span class="number">0.106</span>  <span class="number">0.175</span></span><br><span class="line">heroes <span class="number">0.156</span>  <span class="number">0.453</span>  <span class="number">0.028</span>  <span class="number">0.129</span>  <span class="number">0.234</span></span><br><span class="line">  wear <span class="number">0.499</span>  <span class="number">0.055</span>  <span class="number">0.133</span>  <span class="number">0.017</span>  <span class="number">0.295</span></span><br><span class="line"> capes <span class="number">0.089</span>  <span class="number">0.290</span>  <span class="number">0.240</span>  <span class="number">0.228</span>  <span class="number">0.153</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># 因果自注意力 （行为j, 列为i）</span></span><br><span class="line"> <span class="comment"># 为防止输入的所有查询都能预测未来，需要将所有j&gt;i位置设置为0 ：</span></span><br><span class="line">        <span class="keyword">not</span>    <span class="built_in">all</span>   heroes  wear  capes</span><br><span class="line">   <span class="keyword">not</span> <span class="number">0.116</span>  <span class="number">0.</span>     <span class="number">0.</span>     <span class="number">0.</span>     <span class="number">0.</span></span><br><span class="line">   <span class="built_in">all</span> <span class="number">0.180</span>  <span class="number">0.397</span>  <span class="number">0.</span>     <span class="number">0.</span>     <span class="number">0.</span></span><br><span class="line">heroes <span class="number">0.156</span>  <span class="number">0.453</span>  <span class="number">0.028</span>  <span class="number">0.</span>     <span class="number">0.</span></span><br><span class="line">  wear <span class="number">0.499</span>  <span class="number">0.055</span>  <span class="number">0.133</span>  <span class="number">0.017</span>  <span class="number">0.</span></span><br><span class="line"> capes <span class="number">0.089</span>  <span class="number">0.290</span>  <span class="number">0.240</span>  <span class="number">0.228</span>  <span class="number">0.153</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># 在应用 softmax 之前，我们需要修改我们的注意力矩阵，得到掩码自注意力</span></span><br><span class="line"> <span class="comment"># 即，在softmax之前将要屏蔽项的注意力得分设置为 −∞（归一化系数为0）</span></span><br><span class="line"> <span class="comment"># mask掩码矩阵</span></span><br><span class="line"> <span class="number">0</span> -<span class="number">1e10</span> -<span class="number">1e10</span> -<span class="number">1e10</span> -<span class="number">1e10</span></span><br><span class="line"> <span class="number">0</span>   <span class="number">0</span>   -<span class="number">1e10</span> -<span class="number">1e10</span> -<span class="number">1e10</span></span><br><span class="line"> <span class="number">0</span>   <span class="number">0</span>     <span class="number">0</span>   -<span class="number">1e10</span> -<span class="number">1e10</span></span><br><span class="line"> <span class="number">0</span>   <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>   -<span class="number">1e10</span></span><br><span class="line"> <span class="number">0</span>   <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span>     <span class="number">0</span></span><br><span class="line"></span><br><span class="line"> 使用 -<span class="number">1e10</span> 而不是 -np.inf ，因为 -np.inf 可能会导致 nans</span><br></pre></td></tr></table></figure></li>
</ul>
<p>至此，整个 GPT 的结构拆解完毕。</p>

                    
                </div>

                
                        
<div class="post-copyright-info-container border-box">
    <div class="copyright-info-content border-box">
        <div class="copyright-info-top border-box">
            <div class="copyright-post-title border-box text-ellipsis">
                【LLM】nanoGPT学习
            </div>

            <div class="copyright-post-link border-box text-ellipsis">
                LLM/nanogpt/
            </div>
        </div>

        <div class="copyright-info-bottom border-box">
            <div class="copyright-post-author bottom-item">
                <div class="type">
                    作者
                </div>
                <div class="content">wbfwonderful</div>
            </div>

            <div class="post-time bottom-item">
                <div class="type">
                    发布于
                </div>
                <div class="content">2025-02-23 14:06</div>
            </div>


            <div class="post-license bottom-item">
                <div class="type">
                    许可
                </div>
                <div class="content tooltip" data-tooltip-content="CC BY-NC-SA 4.0">
                    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" target="_blank">
                        
                            <i class="fa-brands fa-creative-commons"></i>
                            <i class="fa-brands fa-creative-commons-by"></i>
                            <i class="fa-brands fa-creative-commons-nc"></i>
                            <i class="fa-brands fa-creative-commons-sa"></i>
                        
                    </a>
                </div>
            </div>
        </div>

        <i class="copyright-bg fa-solid fa-copyright"></i>
    </div>
    <div class="copy-copyright-info flex-center tooltip" data-tooltip-content="复制版权信息" data-tooltip-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/blog/tags/LLM/">LLM</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                            <div class="post-share-container border-box">
    <ul class="share-list-wrap border-box">
        <li class="qq share-item border-box flex-center tooltip"
            data-tooltip-content="分享到 QQ"
        >
            <i class="fa-brands fa-qq"></i>
        </li>
        <li class="wechat share-item border-box flex-center tooltip tooltip-img"
            data-tooltip-content="分享到微信"
            data-tooltip-img-tip="微信扫一扫"
            data-tooltip-img-style="background-color: #fff; top: -10px; padding: 0.6rem 0.6rem 0.1rem 0.6rem;"
        >
            <i class="fa-brands fa-weixin"></i>
        </li>
        <li class="weibo share-item border-box flex-center tooltip"
            data-tooltip-content="分享到微博"
        >
            <i class="fa-brands fa-weibo"></i>
        </li>
    </ul>
</div>

                        
                    </div>
                </div>

                

                
                    <div class="post-nav border-box">
                        
                            <div class="prev-post">
                                <a class="prev"
                                   rel="prev"
                                   href="/blog/ML2025/gen-AI/"
                                   title="【ML2025】1-生成式人工智能"
                                >
                                    <span class="left arrow-icon flex-center">
                                        <i class="fas fa-chevron-left"></i>
                                    </span>
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">【ML2025】1-生成式人工智能</span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/blog/Mark/debug/"
                                   title="Pycharm debug 方法记录"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">Pycharm debug 方法记录</span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
            <div class="pc-post-toc right-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPT-%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">GPT 原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-token"><span class="nav-number">2.1.</span> <span class="nav-text">关于 token</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-%E7%B1%BB"><span class="nav-number">3.1.</span> <span class="nav-text">GPT 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">3.1.1.</span> <span class="nav-text">整体结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#token-%E7%BC%96%E7%A0%81%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">3.1.2.</span> <span class="nav-text">token 编码和位置编码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Block-%E7%B1%BB"><span class="nav-number">3.2.</span> <span class="nav-text">Block 类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CausalSelfAttention-%E7%B1%BB"><span class="nav-number">3.3.</span> <span class="nav-text">CausalSelfAttention 类</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>
        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="copyright-info info-item">
    &copy;&nbsp;<span>2024</span>&nbsp;-&nbsp;2025
    
            &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/blog/">wbfwonderful</a>
        
    </div>

    <div class="theme-info info-item">
        由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
    </div>

    
        
        <div class="deploy-info info-item">
            
                <a target="_blank" rel="nofollow" href="https://github.com">
            
            本站由 <span class="tooltip" data-tooltip-content="GitHub Pages"><img src="/blog/images/brands/github.png"></span> 提供部署服务
            
                </a>
            
        </div>
    

    
        <div class="count-info info-item">
            
                <span class="count-item border-box word">
                    <span class="item-type border-box">总字数</span>
                    <span class="item-value border-box word">60.4k</span>
                </span>
            

            
                <span class="count-item border-box uv">
                    <span class="item-type border-box">访客数</span>
                    <span class="item-value border-box uv" id="busuanzi_value_site_uv"></span>
                </span>
            

            
                <span class="count-item border-box pv">
                    <span class="item-type border-box">访问量</span>
                    <span class="item-value border-box pv" id="busuanzi_value_site_pv"></span>
                </span>
            
        </div>
    

    
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="post-tools-list border-box">
        <!-- PC encrypt again -->
        

        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        

        <!-- PC full screen -->
        <li class="tools-item flex-center full-screen">
            <i class="fa-solid fa-expand"></i>
        </li>
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <!-- toggle mode -->
        
            <li class="tools-item tool-toggle-theme-mode flex-center">
                <i class="fas fa-moon"></i>
            </li>
        

        <!-- rss -->
        

        <!-- to bottom -->
        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPT-%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">GPT 原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-token"><span class="nav-number">2.1.</span> <span class="nav-text">关于 token</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-%E7%B1%BB"><span class="nav-number">3.1.</span> <span class="nav-text">GPT 类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">3.1.1.</span> <span class="nav-text">整体结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#token-%E7%BC%96%E7%A0%81%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">3.1.2.</span> <span class="nav-text">token 编码和位置编码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Block-%E7%B1%BB"><span class="nav-number">3.2.</span> <span class="nav-text">Block 类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CausalSelfAttention-%E7%B1%BB"><span class="nav-number">3.3.</span> <span class="nav-text">CausalSelfAttention 类</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>





<!-- common js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/utils.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/header-shrink.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/back2top.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/toggle-theme.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/code-block.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/main.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/libs/anime.min.js"></script>

<!-- local search -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/local-search.min.js"></script>


<!-- lazyload -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/lazyload.min.js"></script>


<div class="">
    <!-- home page -->
    

    <!-- post page -->
    
        <!-- post-helper -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/post/post-helper.min.js"></script>

        <!-- toc -->
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/post/toc.min.js"></script>
        

        <!-- copyright-info -->
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/post/copyright-info.min.js"></script>
        

        <!-- share -->
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-theme-keep/4.2.4/js/post/share.min.js"></script>
        
    

    <!-- categories page -->
    

    <!-- links page -->
    

    <!-- photos page -->
    

    <!-- tools page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



</body>
</html>
